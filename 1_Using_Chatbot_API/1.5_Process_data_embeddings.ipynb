{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input text into chunks of specified size (1024)\n",
    "def split_into_chunks(text, chunk_size=1024):\n",
    "  chunks = []\n",
    "  for i in range(0, len(text), chunk_size):\n",
    "    chunks.append(text[i:i+chunk_size])\n",
    "\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of articles: 14\n",
      "number of chunks: 174\n"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "import csv\n",
    "\n",
    "chunks = []\n",
    "\n",
    "# Load the file as a CSV\n",
    "with open(\"./1_Using_Chatbot_API/mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "  csv_reader = csv.reader(file, delimiter=\"\\t\")\n",
    "\n",
    "  for idx, row in enumerate( csv_reader ):\n",
    "    if idx == 0: continue; # Skip header row\n",
    "    chunks.extend(split_into_chunks(row[1]))\n",
    "\n",
    "print(\"number of articles:\", idx)\n",
    "print(\"number of chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['chunk'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list to a Pandas Dataframe\n",
    "df = pd.DataFrame(chunks, columns=['chunk'])\n",
    "\n",
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data embeddings using the OpenAI text-embedding-3-small model\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Defining a function that converts a text to embedding vector using OpenAI's Ada model.\n",
    "def get_embedding(text):\n",
    "  try:\n",
    "    # Remove newlines\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    res = client.embeddings.create(input=[text], model=\"text-embedding-3-small\")\n",
    "\n",
    "    return res.data[0].embedding\n",
    "\n",
    "  except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fc442e018f4e8f884fbdfa6ef33473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Generate embedding\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = []\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "  embeddings.append(get_embedding(row['chunk']))\n",
    "\n",
    "# Add the \"embedding\" column to the dataframe\n",
    "embeddings_values = pd.Series(embeddings)\n",
    "df.insert(loc=1, column='embedding', value=embeddings_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46773341 0.46912591 0.25978152 0.2938158  0.31967458 0.40164521\n",
      "  0.41504525 0.4525753  0.45929084 0.12604131 0.11753091 0.01344322\n",
      "  0.2260097  0.2142525  0.10143629 0.33072012 0.10745194 0.34694871\n",
      "  0.16311813 0.08741076 0.34824215 0.22839518 0.19205032 0.26476001\n",
      "  0.24955816 0.34833881 0.24834228 0.32762574 0.41444235 0.41335705\n",
      "  0.46364893 0.38345735 0.46855645 0.35642136 0.35398538 0.30275087\n",
      "  0.2994191  0.29257011 0.40031753 0.46468319 0.3947144  0.41046847\n",
      "  0.44707962 0.43177834 0.35912069 0.33981274 0.51355581 0.2092876\n",
      "  0.40203406 0.32830316 0.4283271  0.48267992 0.45033212 0.3425906\n",
      "  0.32084533 0.42600947 0.24656291 0.18087562 0.2366496  0.34272949\n",
      "  0.34377754 0.20473187 0.19763453 0.22442031 0.21108372 0.42298466\n",
      "  0.26382744 0.30427213 0.33608375 0.38371545 0.23529731 0.24347982\n",
      "  0.37074994 0.28020178 0.49052503 0.53047743 0.3782057  0.4377435\n",
      "  0.37767354 0.39259992 0.30086669 0.41712126 0.46747369 0.45419194\n",
      "  0.35156058 0.21228866 0.42623473 0.31603508 0.44058488 0.52727785\n",
      "  0.50599529 0.49751443 0.44284556 0.35114649 0.39483491 0.44134527\n",
      "  0.20328705 0.27926597 0.15404483 0.19228087 0.15912351 0.2410772\n",
      "  0.22523484 0.19943632 0.26231146 0.35060261 0.3621904  0.15316608\n",
      "  0.27645759 0.45338808 0.33432186 0.29444112 0.38160578 0.4172119\n",
      "  0.61953101 0.38690114 0.34437145 0.28275648 0.20158952 0.14611004\n",
      "  0.19516071 0.28226726 0.15624049 0.18058744 0.30279851 0.28139205\n",
      "  0.30261309 0.23782989 0.14549918 0.19745894 0.39240474 0.33000759\n",
      "  0.23545656 0.1570537  0.26887607 0.26478377 0.37819151 0.18126983\n",
      "  0.13047551 0.18455338 0.26054357 0.35592299 0.33277615 0.23504426\n",
      "  0.37103824 0.19002948 0.18963116 0.20083951 0.16392139 0.3500949\n",
      "  0.2524212  0.33872521 0.18290838 0.30638536 0.24204104 0.13083708\n",
      "  0.18218162 0.19086746 0.41316022 0.16360567 0.26365129 0.20633043\n",
      "  0.30182374 0.24800064 0.41007405 0.21778566 0.22249178 0.27770754\n",
      "  0.14573012 0.19763099 0.35371152 0.15400485 0.32276182 0.30300924]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# The question we want to ask the model.\n",
    "QUESTION = \"How many parameters LLaMA2 model has?\"\n",
    "QUESTION_emb = get_embedding(QUESTION)\n",
    "\n",
    "# The similarity between the questions and each part of the essay.\n",
    "cosine_similarities = cosine_similarity([QUESTION_emb], df['embedding'].tolist())\n",
    "\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114  75  89]\n"
     ]
    }
   ],
   "source": [
    "# Find the N highest scored chunks\n",
    "import numpy as np\n",
    "\n",
    "number_of_chunks_to_retrieve = 3\n",
    "\n",
    "# Sort and find the index of N highest scored chunks\n",
    "indices = np.argsort(cosine_similarities[0])[::-1][:number_of_chunks_to_retrieve]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Chunk 1\n",
      "by Meta that ventures into both the AI and academic spaces. The model aims to help researchers, scientists, and engineers advance their work in exploring AI applications. It will be released under a non-commercial license to prevent misuse, and access will be granted to academic researchers, individuals, and organizations affiliated with the government, civil society, academia, and industry research facilities on a selective case-by-case basis. The sharing of codes and weights allows other researchers to test new approaches in LLMs. The LLaMA models have a range of 7 billion to 65 billion parameters. LLaMA-65B can be compared to DeepMind's Chinchilla and Google's PaLM. Publicly available unlabeled data was used to train these models, and training smaller foundational models require less computing power and resources. LLaMA 65B and 33B have been trained on 1.4 trillion tokens in 20 different languages, and according to the Facebook Artificial Intelligence Research (FAIR) team, the model's performance varies ac\n",
      "----\n",
      "> Chunk 2\n",
      "LLaMA: Meta's new AI tool According to the official release, LLaMA is a foundational language model developed to assist 'researchers and academics' in their work (as opposed to the average web user) to understand and study these NLP models. Leveraging AI in such a way could give researchers an edge in terms of time spent. You may not know this, but this would be Meta's third LLM after Blender Bot 3 and Galactica. However, the two LLMs were shut down soon, and Meta stopped their further development, as it produced erroneous results. Before moving further, it is important to emphasize that LLaMA is NOT a chatbot like ChatGPT. As I mentioned before, it is a 'research tool' for researchers. We can expect the initial versions of LLaMA to be a bit more technical and indirect to use as opposed to the case with ChatGPT, which was very direct, interactive, and a lot easy to use. \"Smaller, more performant models such as LLaMA enable ... research community who don't have access to large amounts of infrastructure to stud\n",
      "----\n",
      "> Chunk 3\n",
      "I. Llama 2: Revolutionizing Commercial Use Unlike its predecessor Llama 1, which was limited to research use, Llama 2 represents a major advancement as an open-source commercial model. Businesses can now integrate Llama 2 into products to create AI-powered applications. Availability on Azure and AWS facilitates fine-tuning and adoption. However, restrictions apply to prevent exploitation. Companies with over 700 million active daily users cannot use Llama 2. Additionally, its output cannot be used to improve other language models.  II. Llama 2 Model Flavors Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters. While 7B, 13B, and 70B have already been released, the 34B model is still awaited. The pretrained variant, trained on a whopping 2 trillion tokens, boasts a context window of 4096 tokens, twice the size of its predecessor Llama 1. Meta also released a Llama 2 fine-tuned model for chat applications that was trained on over 1 million human annota\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Look at the highest scored retrieved pieces of text\n",
    "for idx, item in enumerate(df.chunk[indices]):\n",
    "  print(f\"> Chunk {idx+1}\")\n",
    "  print(item)\n",
    "  print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mini-llama-articles-with-embeddings.csv\", index=False, sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
